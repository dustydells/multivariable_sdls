{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning, Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a basic neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This section creates a linear neural network and grabs its weights. \n",
    "# Read in the data. \n",
    "wells = pd.read_csv('wells.csv')\n",
    "wells.head()\n",
    "\n",
    "# Create a network with 1 linear unit\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(units=1, input_shape=[1]) # Units defines how many outputs you want. Input_shape is the dimensions of the input.\n",
    "])\n",
    "\n",
    "# Get the weights.\n",
    "w, b = model.weights\n",
    "\n",
    "print(f'Weights\\n{w}\\n\\nBias\\n{b}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a neural network with multiple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # Hidden ReLU layers\n",
    "    layers.Dense(units=1, activation='relu', input_shape=[1]),\n",
    "    layers.Dense(units=1, activation='relu'),\n",
    "    # Linear output layer\n",
    "    layers.Dense(units=1)\n",
    "])\n",
    "# You might wanna check the numbers because I changed them all to be 1 in order to fit the wells data. \n",
    "# 'relu' makes a chonky redirection. 'swish' is cool bc it's smooth. There's also 'elu' and 'selu.'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Early stopping can stop the machine learning model before the prediction gets too noisy (when loss is no longer decreasing)\n",
    "# If there hasn't been at least an improvement of .001 over the last 20 epochs, stop training.\n",
    "early_stopping = EarlyStopping(\n",
    "    min_delta=.001, # min amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights = True, # Keep the weights with the best loss. Keep the best model.\n",
    ")\n",
    "# Put this code before you define your model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent (loss optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mae'\n",
    ")\n",
    "\n",
    "\n",
    "# Split the model.\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "\n",
    "# Train the data.\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=256, # Feed optimizer 256 rows of training data at a time.\n",
    "    epochs=10, # Do that 10 times all the way through the dataset.\n",
    "\n",
    "    callbacks=[early_stopping], # Stick in early stopping as a list for the parameter.\n",
    "    verbose=0, # Turn off training log. Or suppress output.\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the loss is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert training history to a dataframe.\n",
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "# Use Pandas native plot method.\n",
    "history_df['loss'].plot()\n",
    "\n",
    "# This time plot is with validation loss included. \n",
    "history_df.loc[:, ['loss', 'val_loss']].plot(); # donno what's up with this weird syntax\n",
    "print(f'Minimum validation loss: {history_df['val_loss'].min()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout (for avoiding underfitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keras.Sequential([\n",
    "    layers.Dense(units=1, activation='relu', input_shape=[1]),\n",
    "    # Stick in a dropout at a rate of 30% to the following layer.\n",
    "    layers.Dropout(rate=.3), \n",
    "    layers.Dense(units=1, activation='relu'),\n",
    "    layers.Dense(units=1)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
